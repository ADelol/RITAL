{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "### APPRENDRE\n",
    "\n",
    "#Prepare training data\n",
    "fname = './president/corpus.tache1.learn.utf8'\n",
    "f=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "txts=f.readlines()\n",
    "nblignes=len(txts)\n",
    "\n",
    "#cptM= 0\n",
    "#cptC=0\n",
    "labels = [] # M:-1, C:1\n",
    "trainData = []\n",
    "for t in txts:\n",
    "    label = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",t)\n",
    "    txt = re.sub(r\"<[0-9]*:[0-9]*:.> (.*)\",\"\\\\1\",t)\n",
    "    \n",
    "    trainData.append(txt)   \n",
    "    \n",
    "    if label[0]==\"C\":\n",
    "        #cptC+=1\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        #cptM +=1\n",
    "        labels.append(-1)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "#Pre-processing\n",
    "def clean_text(textData,labels):\n",
    "    list_stopwords = stopwords.words('french')\n",
    "    list_stopwords.extend(['quand','avoir','être','là','où','à','donc','aucun','aucune','car','a','ici','plus','comment','abord','toujours','très','celui','celle','celles','encore','autre']) #merci?\n",
    "    list_stopwords.extend(['bien','vers','déjà','aussi','devoir','falloir','monsieur','madame','messieurs','mesdames','mademoiselle','mesdemoiselles','tout','faire','pouvoir','aussi','comme','dire','al','entre','sans','aujourdhui','si','cela','deux','dont','aller','chacun','chacune','an','depuis','venir','beaucoup','parce','notamment','quelque','chaque','contre','ainsi','voir','lequel','laquelle','lesquels','lesquelles','apresmidi','vouloir'])\n",
    "    #print(list_stopwords)\n",
    "    \n",
    "    #stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "    \n",
    "    trainC=[]\n",
    "    trainM=[]\n",
    "    for i in range(len(textData)):\n",
    "        text= re.sub(r\"\\n\", \" \", textData[i])  #remove \"\\n\"\n",
    "        #text= re.sub(r\"'\", \" \", text)  #remove '\n",
    "        \n",
    "        doc=nlp(text)\n",
    "        text=' '.join(token.lemma_ for token in doc)\n",
    "        \n",
    "        pure_text = ''\n",
    "        for letter in text:\n",
    "            if letter.isalpha() or letter==' ': #remove punctuation\n",
    "                pure_text += letter.lower() #lower case     \n",
    "        #text = ' '.join(stemmer.stem(word) for word in pure_text.split() if word not in list_stopwords) #racinisation   \n",
    "        text = [word for word in pure_text.split() if word not in list_stopwords] #remove stopwords\n",
    "             \n",
    "        textData[i]=text\n",
    "        if labels[i]==1: #C\n",
    "            trainC.append(text)\n",
    "        else:\n",
    "            trainM.append(text)\n",
    "        \n",
    "    return textData,trainC,trainM\n",
    "\n",
    "\n",
    "train_pure,trainC,trainM=clean_text(trainData,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "\n",
    "dictionary= corpora.Dictionary(train_pure)\n",
    "corpus= [dictionary.doc2bow(text) for text in train_pure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "\n",
    "dictionary_all = corpora.Dictionary(train_pure)\n",
    "corpus_all = [dictionary_all.doc2bow(text) for text in train_pure]\n",
    "#print(dictionary.token2id)\n",
    "#print(corpus)\n",
    "dictionaryC = corpora.Dictionary(trainC)\n",
    "corpusC = [dictionaryC.doc2bow(text) for text in trainC]\n",
    "dictionaryM = corpora.Dictionary(trainM)\n",
    "corpusM = [dictionaryM.doc2bow(text) for text in trainM]\n",
    "\n",
    "\n",
    "lda_all = gensim.models.ldamodel.LdaModel(corpus=corpus_all, id2word=dictionary_all, num_topics=10)\n",
    "topics_all=lda_all.print_topics(num_topics=10, num_words=30)\n",
    "ldaC = gensim.models.ldamodel.LdaModel(corpus=corpusC, id2word=dictionaryC, num_topics=10)\n",
    "topicsC=ldaC.print_topics(num_topics=10, num_words=30)\n",
    "ldaM = gensim.models.ldamodel.LdaModel(corpus=corpusM, id2word=dictionaryM, num_topics=10)\n",
    "topicsM=ldaM.print_topics(num_topics=10, num_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(topics):\n",
    "    res=[]\n",
    "    for t in topics:\n",
    "        dict_tmp={}\n",
    "        tmp=t[1].split(\" + \")\n",
    "        tmp_splited=[r.split('*') for r in tmp]\n",
    "        for ts in tmp_splited:\n",
    "            dict_tmp[ts[1][1:-1]]=ts[0]\n",
    "        res.append(dict_tmp)\n",
    "    return res\n",
    "    \n",
    "dictTopics_all=extract_topics(topics_all)\n",
    "dictTopicsC=extract_topics(topicsC)\n",
    "dictTopicsM=extract_topics(topicsM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freq(dictTopics):\n",
    "    norm=[]\n",
    "    for d in dictTopics:\n",
    "        normFreq={}\n",
    "        somme=sum(float(v) for v in d.values())\n",
    "        for k in d.keys():\n",
    "            normFreq[k]=float(d[k])/somme\n",
    "        norm.append(normFreq)\n",
    "    return norm\n",
    "\n",
    "norm_dict_all=normalize_freq(dictTopics_all)\n",
    "norm_dictC=normalize_freq(dictTopicsC)\n",
    "norm_dictM=normalize_freq(dictTopicsM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_wordCloud(dicts,name):\n",
    "    for i in range(len(dicts)):        \n",
    "        wc = WordCloud(background_color='white',width=400,height=300,max_font_size=50,min_font_size=10,mode='RGBA')\n",
    "        if len(dicts[i])>0:\n",
    "            wc.generate_from_frequencies(dicts[i])\n",
    "        else:\n",
    "            wc.generate_from_frequencies({\"NONE\":1})\n",
    "        filename=\"./img/\"+name+\"/topic\"+str(i)+\".png\"\n",
    "        wc.to_file(filename)\n",
    "        \"\"\"\n",
    "        plt.imshow(wc)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "draw_wordCloud(norm_dict_all,\"withFunctionWord/all_10topics_30words\")\n",
    "draw_wordCloud(norm_dictC,\"withFunctionWord/C_10topics_30words\")\n",
    "draw_wordCloud(norm_dictM,\"withFunctionWord/M_10topics_30words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_functionWord(dictTopics,lda,dictionary,probaMin,limit):\n",
    "    res=[]\n",
    "    for d in dictTopics:\n",
    "        dic={}\n",
    "        for k in d.keys():\n",
    "            if len(lda.get_term_topics(dictionary.token2id[k],minimum_probability=probaMin))<limit:\n",
    "                dic[k]=d[k]\n",
    "        res.append(dic)    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supprimer les mots qui apparaissent avec une forte proba(0.002) dans la plupart des clusters(ici >=3)\n",
    "rmvFW_all=remove_functionWord(dictTopics_all,lda_all,dictionary_all,0.001,3)\n",
    "rmvFW_C=remove_functionWord(dictTopicsC,ldaC,dictionaryC,0.001,3)\n",
    "rmvFW_M=remove_functionWord(dictTopicsM,ldaM,dictionaryM,0.001,3)\n",
    "\n",
    "rmvFW_norm_dict_all=normalize_freq(rmvFW_all)\n",
    "rmvFW_norm_dictC=normalize_freq(rmvFW_C)\n",
    "rmvFW_norm_dictM=normalize_freq(rmvFW_M)\n",
    "\n",
    "draw_wordCloud(rmvFW_norm_dict_all,\"rmvFunctionWord/all_10topics_30words\")\n",
    "draw_wordCloud(rmvFW_norm_dictC,\"rmvFunctionWord/C_10topics_30words\")\n",
    "draw_wordCloud(rmvFW_norm_dictM,\"rmvFunctionWord/M_10topics_30words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chercher les mots discriminants dans un cluster mais absent dans un autre\n",
    "\n",
    "#avec 30mots, possible d'avoir des ensembles vide\n",
    "#[{'cancer': '0.006', 'élection': '0.005', 'municipal': '0.005'}, {}, {}, {}, {'prix': '0.007'}, {}, {}, {}, {'dix': '0.006'}, {}]\n",
    "#donc, on a besoin de prendre plus de mots\n",
    "more_topics_all=lda_all.print_topics(num_topics=10, num_words=50)\n",
    "more_topicsC=ldaC.print_topics(num_topics=10, num_words=50)\n",
    "more_topicsM=ldaM.print_topics(num_topics=10, num_words=50)\n",
    "\n",
    "more_dictTopics_all=extract_topics(more_topics_all)\n",
    "more_dictTopicsC=extract_topics(more_topicsC)\n",
    "more_dictTopicsM=extract_topics(more_topicsM)\n",
    "\n",
    "discriminant_all=remove_functionWord(more_dictTopics_all,lda_all,dictionary_all,0,2)\n",
    "discriminantC=remove_functionWord(more_dictTopicsC,ldaC,dictionaryC,0,2)\n",
    "discriminantM=remove_functionWord(more_dictTopicsM,ldaM,dictionaryM,0,2)\n",
    "\n",
    "norm_discriminant_all=normalize_freq(discriminant_all)\n",
    "norm_discriminantC=normalize_freq(discriminantC)\n",
    "norm_discriminantM=normalize_freq(discriminantM)\n",
    "\n",
    "draw_wordCloud(norm_discriminant_all,\"discriminant/all_10topics_50words\")\n",
    "draw_wordCloud(norm_discriminantC,\"discriminant/C_10topics_50words\")\n",
    "draw_wordCloud(norm_discriminantM,\"discriminant/M_10topics_50words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "#Prepare training data\n",
    "word2_data=[]\n",
    "for i in range(nblignes):\n",
    "    if labels[i]==1: # M:-1, C:1\n",
    "        word2_data.append(\"Chirac: \"+trainData[i])\n",
    "    else:\n",
    "        word2_data.append(\"Mitterrand: \"+trainData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "#Pre-processing\n",
    "def word2_clean_text(textData):\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "    \n",
    "    for i in range(len(textData)):\n",
    "        text= re.sub(r\"\\n\", \" \", textData[i])  #remove \"\\n\"\n",
    "        \n",
    "        doc=nlp(text)\n",
    "        text=' '.join(token.lemma_ for token in doc) #lemmatization\n",
    "        \n",
    "        pure_text = ''\n",
    "        for letter in text:\n",
    "            if letter.isalpha() or letter==' ': #remove punctuation\n",
    "                pure_text += letter.lower() #lower case     \n",
    "        text = [word for word in pure_text.split()]\n",
    "             \n",
    "        textData[i]=text\n",
    "        \n",
    "    return textData\n",
    "\n",
    "word2_pure=word2_clean_text(word2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(word2_pure,sg=1,size=100,window=5,min_count=5,workers=4)\n",
    "model.save(\"president_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mitterrand', 0.7210680842399597),\n",
       " ('collectivement', 0.6074990034103394),\n",
       " ('auxquels', 0.603534460067749),\n",
       " ('persuadé', 0.5978818535804749),\n",
       " ('vigiler', 0.5830539464950562),\n",
       " ('suggérer', 0.5809817314147949),\n",
       " ('chicago', 0.5804547667503357),\n",
       " ('souhaitais', 0.5804014205932617),\n",
       " ('démonstration', 0.5801637768745422),\n",
       " ('accompli', 0.5794323682785034),\n",
       " ('réussi', 0.5785024762153625),\n",
       " ('convaincr', 0.5783129930496216),\n",
       " ('magnifier', 0.5763499140739441),\n",
       " ('redi', 0.576248824596405),\n",
       " ('termine', 0.5754048824310303),\n",
       " ('nigeria', 0.5749176144599915),\n",
       " ('pluriel', 0.5722047686576843),\n",
       " ('reconnaisser', 0.570411205291748),\n",
       " ('néanmoins', 0.5693356990814209),\n",
       " ('initiateur', 0.568634033203125),\n",
       " ('pardonner', 0.5686154365539551),\n",
       " ('incarnez', 0.5678385496139526),\n",
       " ('glasgow', 0.5673726201057434),\n",
       " ('convaincu', 0.5672576427459717),\n",
       " ('retrouvon', 0.5667624473571777),\n",
       " ('parachever', 0.5662547945976257),\n",
       " ('que', 0.5657082200050354),\n",
       " ('habité', 0.5651386380195618),\n",
       " ('chinon', 0.5651324987411499),\n",
       " ('séduire', 0.5645695924758911),\n",
       " ('retenir', 0.5639837980270386),\n",
       " ('comprend', 0.563117265701294),\n",
       " ('imiter', 0.5629215836524963),\n",
       " ('fermement', 0.5620830655097961),\n",
       " ('csce', 0.5618796944618225),\n",
       " ('cheviller', 0.5618442893028259),\n",
       " ('remarquablement', 0.5618409514427185),\n",
       " ('pourrai', 0.5618093013763428),\n",
       " ('symboliquement', 0.5618076324462891),\n",
       " ('méritent', 0.5616154670715332),\n",
       " ('proviseur', 0.559562087059021),\n",
       " ('êtes', 0.5592440366744995),\n",
       " ('quinquennat', 0.5590654611587524),\n",
       " ('solennellement', 0.5589797496795654),\n",
       " ('croyon', 0.558783233165741),\n",
       " ('aurai', 0.558447539806366),\n",
       " ('sache', 0.5580364465713501),\n",
       " ('puisse', 0.5572923421859741),\n",
       " ('vivon', 0.5571467876434326),\n",
       " ('appuie', 0.5567885041236877)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "model.wv.similar_by_word('chirac', topn =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chirac', 0.7210680246353149),\n",
       " ('pardonner', 0.6413301229476929),\n",
       " ('redi', 0.6142987608909607),\n",
       " ('cognac', 0.6086660623550415),\n",
       " ('tromper', 0.6048446297645569),\n",
       " ('insiste', 0.6042050719261169),\n",
       " ('reconnais', 0.6024350523948669),\n",
       " ('connai', 0.5973896980285645),\n",
       " ('aussitôt', 0.5968186855316162),\n",
       " ('pourrai', 0.5966140627861023),\n",
       " ('excuser', 0.5958927273750305),\n",
       " ('trancher', 0.5958324074745178),\n",
       " ('attarder', 0.5943892002105713),\n",
       " ('quelqu', 0.5934343934059143),\n",
       " ('confidence', 0.5907344818115234),\n",
       " ('tort', 0.5905795693397522),\n",
       " ('chinon', 0.5904669761657715),\n",
       " ('dirai', 0.5900932550430298),\n",
       " ('fût', 0.5880401134490967),\n",
       " ('regret', 0.5878316164016724),\n",
       " ('volontiers', 0.5865918397903442),\n",
       " ('répéter', 0.586157500743866),\n",
       " ('pardonnez', 0.5858516693115234),\n",
       " ('miracle', 0.5856518745422363),\n",
       " ('ressembler', 0.5842971801757812),\n",
       " ('excuse', 0.5841436386108398),\n",
       " ('fortiori', 0.5821635723114014),\n",
       " ('quoi', 0.5820456147193909),\n",
       " ('retenir', 0.5814182758331299),\n",
       " ('regretter', 0.581161618232727),\n",
       " ('évoque', 0.5802152156829834),\n",
       " ('étonner', 0.5769973993301392),\n",
       " ('provincial', 0.5754430294036865),\n",
       " ('csce', 0.574913740158081),\n",
       " ('suffit', 0.5737360119819641),\n",
       " ('faudrait', 0.5735805034637451),\n",
       " ('rappelle', 0.5712550282478333),\n",
       " ('bayonne', 0.570217490196228),\n",
       " ('étai', 0.5695408582687378),\n",
       " ('arrive', 0.5680199861526489),\n",
       " ('distribuer', 0.5678649544715881),\n",
       " ('dison', 0.567775309085846),\n",
       " ('cache', 0.5673940181732178),\n",
       " ('regrettable', 0.5672474503517151),\n",
       " ('voulez', 0.5672281980514526),\n",
       " ('inconvénient', 0.5665228962898254),\n",
       " ('justification', 0.566501259803772),\n",
       " ('promener', 0.5662856698036194),\n",
       " ('regrette', 0.5659007430076599),\n",
       " ('statistique', 0.5652562975883484)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('mitterrand', topn =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tunisie', 0.3573126792907715),\n",
       " ('solidarité', 0.3236124515533447),\n",
       " ('inlassable', 0.3187448978424072),\n",
       " ('renouveler', 0.3100992739200592),\n",
       " ('participe', 0.30520692467689514),\n",
       " ('universalité', 0.3013571500778198),\n",
       " ('action', 0.30108070373535156),\n",
       " ('neutralité', 0.3000565767288208),\n",
       " ('encouragement', 0.2965849041938782),\n",
       " ('redonner', 0.2915741205215454),\n",
       " ('association', 0.2900245189666748),\n",
       " ('renouveau', 0.2879122495651245),\n",
       " ('édification', 0.2877460718154907),\n",
       " ('efficacité', 0.28675806522369385),\n",
       " ('excellence', 0.28661903738975525),\n",
       " ('enracinement', 0.2854979634284973),\n",
       " ('priorité', 0.28395313024520874),\n",
       " ('renforcemer', 0.2839243710041046),\n",
       " ('solidité', 0.2836388945579529),\n",
       " ('pacte', 0.2827143371105194)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"chirac\"],negative=[\"mitterrand\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on', 0.27397143840789795),\n",
       " ('répéter', 0.246195986866951),\n",
       " ('lire', 0.24089786410331726),\n",
       " ('arrive', 0.2374349981546402),\n",
       " ('arriver', 0.2310277223587036),\n",
       " ('chose', 0.22063590586185455),\n",
       " ('discours', 0.21441566944122314),\n",
       " ('apercevoir', 0.21405036747455597),\n",
       " ('acheter', 0.2096652090549469),\n",
       " ('assez', 0.20324957370758057),\n",
       " ('pire', 0.20241950452327728),\n",
       " ('durer', 0.20227405428886414),\n",
       " ('douze', 0.20203140377998352),\n",
       " ('aussitôt', 0.19896963238716125),\n",
       " ('séparer', 0.19762855768203735),\n",
       " ('sinon', 0.19760464131832123),\n",
       " ('gens', 0.19339323043823242),\n",
       " ('dire', 0.1918984055519104),\n",
       " ('faudrait', 0.18919777870178223),\n",
       " ('finalement', 0.18886950612068176)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"mitterrand\"],negative=[\"chirac\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
